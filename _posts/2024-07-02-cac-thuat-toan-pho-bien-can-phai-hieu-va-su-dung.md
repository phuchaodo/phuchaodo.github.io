---
title: '[Note] CÃ¡c thuáº­t toÃ¡n phá»• biáº¿n cáº§n pháº£i hiá»ƒu vÃ  sá»­ dá»¥ng'
date: 2024-07-02
permalink: /posts/2024/07/02/cac-thuat-toan-pho-bien-can-phai-hieu-va-su-dung/
tags:
  - Algorithm
  - Pytorch
--- 

Hiá»ƒu hÆ¡n vá» cÃ¡c thuáº­t toÃ¡n phá»• biáº¿n cáº§n pháº£i biáº¿t vÃ  sá»­ dá»¥ng


ğŸ­ğŸ¬ ğ—”ğ—¹ğ—´ğ—¼ğ—¿ğ—¶ğ˜ğ—µğ—ºğ˜€ ğ—˜ğ˜ƒğ—²ğ—¿ğ˜† ğ—˜ğ—»ğ—´ğ—¶ğ—»ğ—²ğ—²ğ—¿ ğ—¦ğ—µğ—¼ğ˜‚ğ—¹ğ—± ğ—ğ—»ğ—¼ğ˜„:


ğŸ¬.ğŸ’¡ ğ—•ğ—¿ğ—²ğ—®ğ—±ğ˜ğ—µ-ğ—™ğ—¶ğ—¿ğ˜€ğ˜ ğ—¦ğ—²ğ—®ğ—¿ğ—°ğ—µ (ğ—•ğ—™ğ—¦): 
Explore a graph level by level, starting from the root, which is great for finding the shortest path in unweighted graphs. 
â¡ï¸ Useful when: You're designing web crawlers or analyzing social networks.

ğŸ­.ğŸ’¡ ğ—§ğ˜„ğ—¼ ğ—›ğ—²ğ—®ğ—½ğ˜€: 
Uses a min-heap and max-heap to manage dynamic datasets efficiently, maintaining median and priority. 
â¡ï¸ Useful when: You need to manage a priority queue or dynamic datasets.

ğŸ®.ğŸ’¡ ğ—§ğ˜„ğ—¼ ğ—£ğ—¼ğ—¶ğ—»ğ˜ğ—²ğ—¿ğ˜€: 
This technique takes 2 points in a sequence and performs logic based on the problem.
â¡ï¸ Useful when: You are implementing sorting or searching functions.

ğŸ¯.ğŸ’¡ ğ—¦ğ—¹ğ—¶ğ—±ğ—¶ğ—»ğ—´ ğ—ªğ—¶ğ—»ğ—±ğ—¼ğ˜„: 
Optimizes the computation by reusing the state from the previous subset of data. 
â¡ï¸ Useful when: You're handling network congestion or data compression.

ğŸ°.ğŸ’¡ ğ——ğ—²ğ—½ğ˜ğ—µ-ğ—™ğ—¶ğ—¿ğ˜€ğ˜ ğ—¦ğ—²ğ—®ğ—¿ğ—°ğ—µ (ğ——ğ—™ğ—¦): 
Explores each path to the end, ideal for situations that involve exploring all options like in puzzles. 
â¡ï¸ Useful when: You're working with graph structures or need to generate permutations.

ğŸ±.ğŸ’¡ ğ—§ğ—¼ğ—½ğ—¼ğ—¹ğ—¼ğ—´ğ—¶ğ—°ğ—®ğ—¹ ğ—¦ğ—¼ğ—¿ğ˜: 
Helps in scheduling tasks based on their dependencies. 
â¡ï¸ Useful when: You are determining execution order in project management or compiling algorithms.

ğŸ².ğŸ’¡ ğ— ğ—²ğ—¿ğ—´ğ—² ğ—œğ—»ğ˜ğ—²ğ—¿ğ˜ƒğ—®ğ—¹ğ˜€: 
Optimizes overlapping intervals to minimize the number of intervals. 
â¡ï¸ Useful when: Scheduling resources or managing calendars.

ğŸ³.ğŸ’¡ ğ—•ğ—®ğ—°ğ—¸ğ˜ğ—¿ğ—®ğ—°ğ—¸ğ—¶ğ—»ğ—´: 
It explores all potential solutions systematically and is perfect for solving puzzles and optimization problems. 
â¡ï¸ Useful when: Solving complex logical puzzles or optimizing resource allocations.

ğŸ´.ğŸ’¡ ğ—§ğ—¿ğ—¶ğ—² (ğ—£ğ—¿ğ—²ğ—³ğ—¶ğ˜… ğ—§ğ—¿ğ—²ğ—²): 
A tree-like structure that manages dynamic sets of strings efficiently, often used for searching. 
â¡ï¸ Useful when: Implementing spell-checkers or autocomplete systems.

ğŸµ.ğŸ’¡ ğ—™ğ—¹ğ—¼ğ—¼ğ—± ğ—™ğ—¶ğ—¹ğ—¹: 
It fills a contiguous area for features like the 'paint bucket' tool. 
â¡ï¸ Useful when: Working in graphics editors or game development.

ğŸ­ğŸ¬.ğŸ’¡ ğ—¦ğ—²ğ—´ğ—ºğ—²ğ—»ğ˜ ğ—§ğ—¿ğ—²ğ—²: 
Efficiently manages intervals or segments and is useful for storing information about intervals and querying over them. 
â¡ï¸ Useful when: Dealing with database range queries or statistical calculations.


Äá»ƒ trÃ¬nh bÃ y chi tiáº¿t vá» viá»‡c sá»­ dá»¥ng máº¡ng nÆ¡-ron nhÃ¢n táº¡o (ANN - Artificial Neural Network) trong PyTorch cho bÃ i toÃ¡n phÃ¢n loáº¡i nhá»‹ phÃ¢n vÃ  phÃ¢n loáº¡i nhiá»u lá»›p, mÃ¬nh sáº½ cung cáº¥p cÃ¡c vÃ­ dá»¥ cá»¥ thá»ƒ vÃ  mÃ£ nguá»“n Python.

### Binary Classification

#### Chuáº©n bá»‹ dá»¯ liá»‡u
TrÆ°á»›c tiÃªn, chÃºng ta cáº§n chuáº©n bá»‹ dá»¯ liá»‡u Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh. VÃ­ dá»¥, chÃºng ta sá»­ dá»¥ng dá»¯ liá»‡u tá»« thÆ° viá»‡n sklearn:

```python
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
import numpy as np

# Táº¡o dá»¯ liá»‡u giáº£ láº­p
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)

# Chia dá»¯ liá»‡u thÃ nh táº­p huáº¥n luyá»‡n vÃ  táº­p kiá»ƒm tra
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Chuyá»ƒn Ä‘á»•i thÃ nh tensor trong PyTorch
import torch
from torch.utils.data import TensorDataset, DataLoader

X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.float32)

# Táº¡o DataLoader cho táº­p huáº¥n luyá»‡n
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
```

#### XÃ¢y dá»±ng mÃ´ hÃ¬nh ANN
Sá»­ dá»¥ng PyTorch, chÃºng ta cÃ³ thá»ƒ xÃ¢y dá»±ng mÃ´ hÃ¬nh ANN nhÆ° sau:

```python
import torch.nn as nn
import torch.optim as optim

class ANN(nn.Module):
    def __init__(self, input_dim):
        super(ANN, self).__init__()
        self.fc1 = nn.Linear(input_dim, 64)
        self.fc2 = nn.Linear(64, 1)
        self.sigmoid = nn.Sigmoid()
        
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        x = self.sigmoid(x)
        return x

# Khá»Ÿi táº¡o mÃ´ hÃ¬nh vÃ  cÃ¡c tham sá»‘
input_dim = X.shape[1]  # sá»‘ chiá»u cá»§a dá»¯ liá»‡u Ä‘áº§u vÃ o
model = ANN(input_dim)

# Äá»‹nh nghÄ©a hÃ m loss vÃ  optimizer
criterion = nn.BCELoss()  # Binary Cross Entropy Loss
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Huáº¥n luyá»‡n mÃ´ hÃ¬nh
num_epochs = 20

for epoch in range(num_epochs):
    model.train()
    epoch_loss = 0.0
    for inputs, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs.squeeze(), labels)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
    
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader):.4f}')

# ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh trÃªn táº­p kiá»ƒm tra
model.eval()
with torch.no_grad():
    y_pred = model(X_test_tensor)
    y_pred_class = y_pred.round()
    accuracy = (y_pred_class.eq(y_test_tensor.view_as(y_pred_class)).sum() / len(y_test_tensor)).item()
    print(f'Accuracy on test set: {accuracy:.4f}')
```

### Multi-class Classification hoáº·c Regression (Prediction)

#### Chuáº©n bá»‹ dá»¯ liá»‡u
ChÃºng ta sá»­ dá»¥ng dá»¯ liá»‡u tá»« thÆ° viá»‡n sklearn, nhÆ°ng vá»›i má»™t vÃ­ dá»¥ cÃ³ nhiá»u lá»›p (multi-class):

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import numpy as np

# Load dá»¯ liá»‡u Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Chia dá»¯ liá»‡u thÃ nh táº­p huáº¥n luyá»‡n vÃ  táº­p kiá»ƒm tra
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Chuyá»ƒn Ä‘á»•i thÃ nh tensor trong PyTorch
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.long)  # y_train lÃ  index cá»§a lá»›p
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.long)

# Táº¡o DataLoader cho táº­p huáº¥n luyá»‡n
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
```

#### XÃ¢y dá»±ng mÃ´ hÃ¬nh ANN
```python
import torch.nn as nn
import torch.optim as optim

class ANN(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(ANN, self).__init__()
        self.fc1 = nn.Linear(input_dim, 64)
        self.fc2 = nn.Linear(64, output_dim)
        
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Khá»Ÿi táº¡o mÃ´ hÃ¬nh vÃ  cÃ¡c tham sá»‘
input_dim = X.shape[1]  # sá»‘ chiá»u cá»§a dá»¯ liá»‡u Ä‘áº§u vÃ o
output_dim = len(np.unique(y))  # sá»‘ lá»›p Ä‘áº§u ra
model = ANN(input_dim, output_dim)

# Äá»‹nh nghÄ©a hÃ m loss vÃ  optimizer
criterion = nn.CrossEntropyLoss()  # Cross Entropy Loss
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Huáº¥n luyá»‡n mÃ´ hÃ¬nh
num_epochs = 20

for epoch in range(num_epochs):
    model.train()
    epoch_loss = 0.0
    for inputs, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
    
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader):.4f}')

# ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh trÃªn táº­p kiá»ƒm tra
model.eval()
with torch.no_grad():
    y_pred = model(X_test_tensor)
    _, y_pred_class = torch.max(y_pred, 1)
    accuracy = (y_pred_class.eq(y_test_tensor).sum() / len(y_test_tensor)).item()
    print(f'Accuracy on test set: {accuracy:.4f}')
```

### Tá»•ng káº¿t
TrÃªn Ä‘Ã¢y lÃ  cÃ¡ch sá»­ dá»¥ng máº¡ng nÆ¡-ron nhÃ¢n táº¡o (ANN) trong PyTorch cho bÃ i toÃ¡n phÃ¢n loáº¡i nhá»‹ phÃ¢n vÃ  phÃ¢n loáº¡i nhiá»u lá»›p. MÃ£ nguá»“n Ä‘Ã£ cung cáº¥p bao gá»“m xÃ¢y dá»±ng mÃ´ hÃ¬nh, chuáº©n bá»‹ dá»¯ liá»‡u, Ä‘á»‹nh nghÄ©a hÃ m loss vÃ  optimizer, huáº¥n luyá»‡n mÃ´ hÃ¬nh vÃ  Ä‘Ã¡nh giÃ¡ káº¿t quáº£ trÃªn táº­p kiá»ƒm tra. Báº¡n cÃ³ thá»ƒ Ä‘iá»u chá»‰nh cÃ¡c tham sá»‘ vÃ  cáº¥u trÃºc mÃ´ hÃ¬nh Ä‘á»ƒ phÃ¹ há»£p vá»›i bÃ i toÃ¡n cá»¥ thá»ƒ cá»§a mÃ¬nh.


Äá»ƒ trÃ¬nh bÃ y chi tiáº¿t vÃ  mÃ£ nguá»“n Python sá»­ dá»¥ng máº¡ng perceptron nhiá»u lá»›p (MLP - Multilayer Perceptron) trong PyTorch cho bÃ i toÃ¡n phÃ¢n loáº¡i nhá»‹ phÃ¢n vÃ  phÃ¢n loáº¡i nhiá»u lá»›p, mÃ¬nh sáº½ cung cáº¥p cÃ¡c vÃ­ dá»¥ cá»¥ thá»ƒ.

### Binary Classification

#### Chuáº©n bá»‹ dá»¯ liá»‡u
ChÃºng ta váº«n sá»­ dá»¥ng dá»¯ liá»‡u tá»« thÆ° viá»‡n sklearn Ä‘á»ƒ minh há»a:

```python
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
import numpy as np

# Táº¡o dá»¯ liá»‡u giáº£ láº­p
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)

# Chia dá»¯ liá»‡u thÃ nh táº­p huáº¥n luyá»‡n vÃ  táº­p kiá»ƒm tra
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Chuyá»ƒn Ä‘á»•i thÃ nh tensor trong PyTorch
import torch
from torch.utils.data import TensorDataset, DataLoader

X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.float32)

# Táº¡o DataLoader cho táº­p huáº¥n luyá»‡n
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
```

#### XÃ¢y dá»±ng mÃ´ hÃ¬nh MLP
Sá»­ dá»¥ng PyTorch, chÃºng ta cÃ³ thá»ƒ xÃ¢y dá»±ng mÃ´ hÃ¬nh MLP nhÆ° sau:

```python
import torch.nn as nn
import torch.optim as optim

class MLP(nn.Module):
    def __init__(self, input_dim):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(input_dim, 64)
        self.fc2 = nn.Linear(64, 1)
        self.sigmoid = nn.Sigmoid()
        
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        x = self.sigmoid(x)
        return x

# Khá»Ÿi táº¡o mÃ´ hÃ¬nh vÃ  cÃ¡c tham sá»‘
input_dim = X.shape[1]  # sá»‘ chiá»u cá»§a dá»¯ liá»‡u Ä‘áº§u vÃ o
model = MLP(input_dim)

# Äá»‹nh nghÄ©a hÃ m loss vÃ  optimizer
criterion = nn.BCELoss()  # Binary Cross Entropy Loss
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Huáº¥n luyá»‡n mÃ´ hÃ¬nh
num_epochs = 20

for epoch in range(num_epochs):
    model.train()
    epoch_loss = 0.0
    for inputs, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs.squeeze(), labels)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
    
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader):.4f}')

# ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh trÃªn táº­p kiá»ƒm tra
model.eval()
with torch.no_grad():
    y_pred = model(X_test_tensor)
    y_pred_class = y_pred.round()
    accuracy = (y_pred_class.eq(y_test_tensor.view_as(y_pred_class)).sum() / len(y_test_tensor)).item()
    print(f'Accuracy on test set: {accuracy:.4f}')
```

### Multi-class Classification hoáº·c Regression (Prediction)

#### Chuáº©n bá»‹ dá»¯ liá»‡u
Tiáº¿p tá»¥c sá»­ dá»¥ng dá»¯ liá»‡u tá»« thÆ° viá»‡n sklearn, nhÆ°ng cho má»™t vÃ­ dá»¥ vá»›i nhiá»u lá»›p:

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import numpy as np

# Load dá»¯ liá»‡u Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Chia dá»¯ liá»‡u thÃ nh táº­p huáº¥n luyá»‡n vÃ  táº­p kiá»ƒm tra
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Chuyá»ƒn Ä‘á»•i thÃ nh tensor trong PyTorch
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.long)  # y_train lÃ  index cá»§a lá»›p
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.long)

# Táº¡o DataLoader cho táº­p huáº¥n luyá»‡n
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
```

#### XÃ¢y dá»±ng mÃ´ hÃ¬nh MLP
```python
import torch.nn as nn
import torch.optim as optim

class MLP(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(input_dim, 64)
        self.fc2 = nn.Linear(64, output_dim)
        
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Khá»Ÿi táº¡o mÃ´ hÃ¬nh vÃ  cÃ¡c tham sá»‘
input_dim = X.shape[1]  # sá»‘ chiá»u cá»§a dá»¯ liá»‡u Ä‘áº§u vÃ o
output_dim = len(np.unique(y))  # sá»‘ lá»›p Ä‘áº§u ra
model = MLP(input_dim, output_dim)

# Äá»‹nh nghÄ©a hÃ m loss vÃ  optimizer
criterion = nn.CrossEntropyLoss()  # Cross Entropy Loss
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Huáº¥n luyá»‡n mÃ´ hÃ¬nh
num_epochs = 20

for epoch in range(num_epochs):
    model.train()
    epoch_loss = 0.0
    for inputs, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
    
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader):.4f}')

# ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh trÃªn táº­p kiá»ƒm tra
model.eval()
with torch.no_grad():
    y_pred = model(X_test_tensor)
    _, y_pred_class = torch.max(y_pred, 1)
    accuracy = (y_pred_class.eq(y_test_tensor).sum() / len(y_test_tensor)).item()
    print(f'Accuracy on test set: {accuracy:.4f}')
```

### Tá»•ng káº¿t
TrÃªn Ä‘Ã¢y lÃ  cÃ¡ch sá»­ dá»¥ng máº¡ng perceptron nhiá»u lá»›p (MLP) trong PyTorch cho bÃ i toÃ¡n phÃ¢n loáº¡i nhá»‹ phÃ¢n vÃ  phÃ¢n loáº¡i nhiá»u lá»›p. MÃ£ nguá»“n Ä‘Ã£ cung cáº¥p bao gá»“m xÃ¢y dá»±ng mÃ´ hÃ¬nh, chuáº©n bá»‹ dá»¯ liá»‡u, Ä‘á»‹nh nghÄ©a hÃ m loss vÃ  optimizer, huáº¥n luyá»‡n mÃ´ hÃ¬nh vÃ  Ä‘Ã¡nh giÃ¡ káº¿t quáº£ trÃªn táº­p kiá»ƒm tra. Báº¡n cÃ³ thá»ƒ Ä‘iá»u chá»‰nh cÃ¡c tham sá»‘ vÃ  cáº¥u trÃºc mÃ´ hÃ¬nh Ä‘á»ƒ phÃ¹ há»£p vá»›i bÃ i toÃ¡n cá»¥ thá»ƒ cá»§a mÃ¬nh.


Äá»ƒ trÃ¬nh bÃ y chi tiáº¿t vÃ  mÃ£ nguá»“n Python sá»­ dá»¥ng Máº¡ng NÆ¡-ron TÃ­ch Cháº­p (CNN - Convolutional Neural Networks) trong PyTorch cho bÃ i toÃ¡n phÃ¢n loáº¡i nhá»‹ phÃ¢n vÃ  phÃ¢n loáº¡i nhiá»u lá»›p, mÃ¬nh sáº½ cung cáº¥p cÃ¡c vÃ­ dá»¥ cá»¥ thá»ƒ.

### Binary Classification

#### Chuáº©n bá»‹ dá»¯ liá»‡u
ChÃºng ta sá»­ dá»¥ng dá»¯ liá»‡u tá»« thÆ° viá»‡n sklearn Ä‘á»ƒ minh há»a. ÄÃ¢y lÃ  má»™t vÃ­ dá»¥ vá»›i dá»¯ liá»‡u giáº£ láº­p:

```python
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
import numpy as np

# Táº¡o dá»¯ liá»‡u giáº£ láº­p
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)

# Chia dá»¯ liá»‡u thÃ nh táº­p huáº¥n luyá»‡n vÃ  táº­p kiá»ƒm tra
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Chuyá»ƒn Ä‘á»•i thÃ nh tensor trong PyTorch
import torch
from torch.utils.data import TensorDataset, DataLoader

X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.float32)

# Reshape láº¡i X Ä‘á»ƒ phÃ¹ há»£p vá»›i Ä‘áº§u vÃ o cá»§a CNN (batch_size, channels, height, width)
X_train_tensor = X_train_tensor.view(-1, 1, 20, 1)
X_test_tensor = X_test_tensor.view(-1, 1, 20, 1)

# Táº¡o DataLoader cho táº­p huáº¥n luyá»‡n
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
```

#### XÃ¢y dá»±ng mÃ´ hÃ¬nh CNN
Sá»­ dá»¥ng PyTorch, chÃºng ta cÃ³ thá»ƒ xÃ¢y dá»±ng mÃ´ hÃ¬nh CNN nhÆ° sau:

```python
import torch.nn as nn
import torch.optim as optim

class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.fc1 = nn.Linear(16 * 10 * 1, 64)  # 16 channels * 10 height * 1 width after pooling
        self.fc2 = nn.Linear(64, 1)
        self.sigmoid = nn.Sigmoid()
        
    def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))
        x = x.view(-1, 16 * 10 * 1)
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        x = self.sigmoid(x)
        return x

# Khá»Ÿi táº¡o mÃ´ hÃ¬nh
model = CNN()

# Äá»‹nh nghÄ©a hÃ m loss vÃ  optimizer
criterion = nn.BCELoss()  # Binary Cross Entropy Loss
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Huáº¥n luyá»‡n mÃ´ hÃ¬nh
num_epochs = 20

for epoch in range(num_epochs):
    model.train()
    epoch_loss = 0.0
    for inputs, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs.squeeze(), labels)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
    
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader):.4f}')

# ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh trÃªn táº­p kiá»ƒm tra
model.eval()
with torch.no_grad():
    X_test_tensor = X_test_tensor.view(-1, 1, 20, 1)
    y_pred = model(X_test_tensor)
    y_pred_class = y_pred.round()
    accuracy = (y_pred_class.eq(y_test_tensor.view_as(y_pred_class)).sum() / len(y_test_tensor)).item()
    print(f'Accuracy on test set: {accuracy:.4f}')
```

### Multi-class Classification hoáº·c Regression (Prediction)

#### Chuáº©n bá»‹ dá»¯ liá»‡u
ChÃºng ta tiáº¿p tá»¥c sá»­ dá»¥ng dá»¯ liá»‡u tá»« thÆ° viá»‡n sklearn, nhÆ°ng cho má»™t vÃ­ dá»¥ vá»›i nhiá»u lá»›p:

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import numpy as np

# Load dá»¯ liá»‡u Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Chia dá»¯ liá»‡u thÃ nh táº­p huáº¥n luyá»‡n vÃ  táº­p kiá»ƒm tra
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Chuyá»ƒn Ä‘á»•i thÃ nh tensor trong PyTorch
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.long)  # y_train lÃ  index cá»§a lá»›p
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.long)

# Reshape láº¡i X Ä‘á»ƒ phÃ¹ há»£p vá»›i Ä‘áº§u vÃ o cá»§a CNN (batch_size, channels, height, width)
X_train_tensor = X_train_tensor.view(-1, 1, 4, 1)
X_test_tensor = X_test_tensor.view(-1, 1, 4, 1)

# Táº¡o DataLoader cho táº­p huáº¥n luyá»‡n
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
```

#### XÃ¢y dá»±ng mÃ´ hÃ¬nh CNN
```python
import torch.nn as nn
import torch.optim as optim

class CNN(nn.Module):
    def __init__(self, num_classes):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.fc1 = nn.Linear(16 * 2 * 1, 64)  # 16 channels * 2 height * 1 width after pooling
        self.fc2 = nn.Linear(64, num_classes)
        
    def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))
        x = x.view(-1, 16 * 2 * 1)
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Khá»Ÿi táº¡o mÃ´ hÃ¬nh
num_classes = len(np.unique(y))  # sá»‘ lá»›p Ä‘áº§u ra
model = CNN(num_classes)

# Äá»‹nh nghÄ©a hÃ m loss vÃ  optimizer
criterion = nn.CrossEntropyLoss()  # Cross Entropy Loss
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Huáº¥n luyá»‡n mÃ´ hÃ¬nh
num_epochs = 20

for epoch in range(num_epochs):
    model.train()
    epoch_loss = 0.0
    for inputs, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
    
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader):.4f}')

# ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh trÃªn táº­p kiá»ƒm tra
model.eval()
with torch.no_grad():
    X_test_tensor = X_test_tensor.view(-1, 1, 4, 1)
    y_pred = model(X_test_tensor)
    _, y_pred_class = torch.max(y_pred, 1)
    accuracy = (y_pred_class.eq(y_test_tensor).sum() / len(y_test_tensor)).item()
    print(f'Accuracy on test set: {accuracy:.4f}')
```

### Tá»•ng káº¿t
TrÃªn Ä‘Ã¢y lÃ  cÃ¡ch sá»­ dá»¥ng Máº¡ng NÆ¡-ron TÃ­ch Cháº­p (CNN) trong PyTorch cho bÃ i toÃ¡n phÃ¢n loáº¡i nhá»‹ phÃ¢n vÃ  phÃ¢n loáº¡i nhiá»u lá»›p. MÃ£ nguá»“n Ä‘Ã£ cung cáº¥p bao gá»“m xÃ¢y dá»±ng mÃ´ hÃ¬nh, chuáº©n bá»‹ dá»¯ liá»‡u, Ä‘á»‹nh nghÄ©a hÃ m loss vÃ  optimizer, huáº¥n luyá»‡n mÃ´ hÃ¬nh vÃ  Ä‘Ã¡nh giÃ¡ káº¿t quáº£ trÃªn táº­p kiá»ƒm tra. Báº¡n cÃ³ thá»ƒ Ä‘iá»u chá»‰nh cÃ¡c tham sá»‘ vÃ  cáº¥u trÃºc mÃ´ hÃ¬nh Ä‘á»ƒ phÃ¹ há»£p vá»›i bÃ i toÃ¡n cá»¥ thá»ƒ cá»§a mÃ¬nh.


Äá»ƒ trÃ¬nh bÃ y chi tiáº¿t vÃ  mÃ£ nguá»“n Python sá»­ dá»¥ng Máº¡ng NÆ¡-ron TÃ¡i PhÃ¡t (RNN - Recurrent Neural Networks) trong PyTorch cho bÃ i toÃ¡n phÃ¢n loáº¡i nhá»‹ phÃ¢n vÃ  phÃ¢n loáº¡i nhiá»u lá»›p, mÃ¬nh sáº½ cung cáº¥p cÃ¡c vÃ­ dá»¥ cá»¥ thá»ƒ.

### Binary Classification

#### Chuáº©n bá»‹ dá»¯ liá»‡u
ChÃºng ta sá»­ dá»¥ng dá»¯ liá»‡u tá»« thÆ° viá»‡n sklearn Ä‘á»ƒ minh há»a. ÄÃ¢y lÃ  má»™t vÃ­ dá»¥ vá»›i dá»¯ liá»‡u giáº£ láº­p:

```python
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
import numpy as np

# Táº¡o dá»¯ liá»‡u giáº£ láº­p
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)

# Chia dá»¯ liá»‡u thÃ nh táº­p huáº¥n luyá»‡n vÃ  táº­p kiá»ƒm tra
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Chuyá»ƒn Ä‘á»•i thÃ nh tensor trong PyTorch
import torch
from torch.utils.data import TensorDataset, DataLoader

X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.float32)

# Reshape láº¡i X Ä‘á»ƒ phÃ¹ há»£p vá»›i Ä‘áº§u vÃ o cá»§a RNN (batch_size, seq_len, input_size)
X_train_tensor = X_train_tensor.view(-1, 20, 1)  # seq_len = 20, input_size = 1
X_test_tensor = X_test_tensor.view(-1, 20, 1)

# Táº¡o DataLoader cho táº­p huáº¥n luyá»‡n
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
```

#### XÃ¢y dá»±ng mÃ´ hÃ¬nh RNN
Sá»­ dá»¥ng PyTorch, chÃºng ta cÃ³ thá»ƒ xÃ¢y dá»±ng mÃ´ hÃ¬nh RNN nhÆ° sau:

```python
import torch.nn as nn
import torch.optim as optim

class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(RNN, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
        self.sigmoid = nn.Sigmoid()
        
    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        out, _ = self.rnn(x, h0)
        out = self.fc(out[:, -1, :])  # Láº¥y output cá»§a lá»›p cuá»‘i cÃ¹ng
        out = self.sigmoid(out)
        return out

# Khá»Ÿi táº¡o mÃ´ hÃ¬nh
input_size = 1  # sá»‘ chiá»u cá»§a dá»¯ liá»‡u Ä‘áº§u vÃ o
hidden_size = 32  # sá»‘ nÆ¡-ron áº©n
num_layers = 1  # sá»‘ lá»›p RNN
output_size = 1  # Ä‘áº§u ra cÃ³ 1 nÆ¡-ron vÃ¬ lÃ  bÃ i toÃ¡n nhá»‹ phÃ¢n
model = RNN(input_size, hidden_size, num_layers, output_size)

# Äá»‹nh nghÄ©a hÃ m loss vÃ  optimizer
criterion = nn.BCELoss()  # Binary Cross Entropy Loss
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Huáº¥n luyá»‡n mÃ´ hÃ¬nh
num_epochs = 20

for epoch in range(num_epochs):
    model.train()
    epoch_loss = 0.0
    for inputs, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs.squeeze(), labels)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
    
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader):.4f}')

# ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh trÃªn táº­p kiá»ƒm tra
model.eval()
with torch.no_grad():
    X_test_tensor = X_test_tensor.view(-1, 20, 1)
    y_pred = model(X_test_tensor)
    y_pred_class = y_pred.round()
    accuracy = (y_pred_class.eq(y_test_tensor.view_as(y_pred_class)).sum() / len(y_test_tensor)).item()
    print(f'Accuracy on test set: {accuracy:.4f}')
```

### Multi-class Classification hoáº·c Regression (Prediction)

#### Chuáº©n bá»‹ dá»¯ liá»‡u
ChÃºng ta tiáº¿p tá»¥c sá»­ dá»¥ng dá»¯ liá»‡u tá»« thÆ° viá»‡n sklearn, nhÆ°ng cho má»™t vÃ­ dá»¥ vá»›i nhiá»u lá»›p:

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import numpy as np

# Load dá»¯ liá»‡u Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Chia dá»¯ liá»‡u thÃ nh táº­p huáº¥n luyá»‡n vÃ  táº­p kiá»ƒm tra
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Chuyá»ƒn Ä‘á»•i thÃ nh tensor trong PyTorch
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.long)  # y_train lÃ  index cá»§a lá»›p
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.long)

# Reshape láº¡i X Ä‘á»ƒ phÃ¹ há»£p vá»›i Ä‘áº§u vÃ o cá»§a RNN (batch_size, seq_len, input_size)
X_train_tensor = X_train_tensor.view(-1, X.shape[1], 1)  # seq_len = sá»‘ Ä‘áº·c trÆ°ng cá»§a Iris, input_size = 1
X_test_tensor = X_test_tensor.view(-1, X.shape[1], 1)

# Táº¡o DataLoader cho táº­p huáº¥n luyá»‡n
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
```

#### XÃ¢y dá»±ng mÃ´ hÃ¬nh RNN
```python
import torch.nn as nn
import torch.optim as optim

class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(RNN, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
        
    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        out, _ = self.rnn(x, h0)
        out = self.fc(out[:, -1, :])  # Láº¥y output cá»§a lá»›p cuá»‘i cÃ¹ng
        return out

# Khá»Ÿi táº¡o mÃ´ hÃ¬nh
input_size = 1  # sá»‘ chiá»u cá»§a dá»¯ liá»‡u Ä‘áº§u vÃ o
hidden_size = 32  # sá»‘ nÆ¡-ron áº©n
num_layers = 1  # sá»‘ lá»›p RNN
output_size = len(np.unique(y))  # sá»‘ lá»›p Ä‘áº§u ra
model = RNN(input_size, hidden_size, num_layers, output_size)

# Äá»‹nh nghÄ©a hÃ m loss vÃ  optimizer
criterion = nn.CrossEntropyLoss()  # Cross Entropy Loss
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Huáº¥n luyá»‡n mÃ´ hÃ¬nh
num_epochs = 20

for epoch in range(num_epochs):
    model.train()
    epoch_loss = 0.0
    for inputs, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
    
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader):.4f}')

# ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh trÃªn táº­p kiá»ƒm tra
model.eval()
with torch.no_grad():
    X_test_tensor = X_test_tensor.view(-1, X.shape[1], 1)
    y_pred = model(X_test_tensor)
    _, y_pred_class = torch.max(y_pred, 1)
    accuracy = (y_pred_class.eq(y_test_tensor).sum() / len(y_test_tensor)).item()
    print(f'Accuracy on test set: {accuracy:.4f}')
```

### Tá»•ng káº¿t
TrÃªn Ä‘Ã¢y lÃ  cÃ¡ch sá»­ dá»¥ng Máº¡ng NÆ¡-ron TÃ¡i PhÃ¡t (RNN) trong PyTorch cho bÃ i toÃ¡n phÃ¢n loáº¡i nhá»‹ phÃ¢n vÃ  phÃ¢n loáº¡i nhiá»u lá»›p. MÃ£ nguá»“n Ä‘Ã£ cung cáº¥p bao gá»“m xÃ¢y dá»±ng mÃ´ hÃ¬nh, chuáº©n bá»‹ dá»¯ liá»‡u, Ä‘á»‹nh nghÄ©a hÃ m loss vÃ  optimizer, huáº¥n luyá»‡n mÃ´ hÃ¬nh vÃ  Ä‘Ã¡nh giÃ¡ káº¿t quáº£ trÃªn táº­p kiá»ƒm tra. Báº¡n cÃ³ thá»ƒ Ä‘iá»u chá»‰nh cÃ¡c tham sá»‘ vÃ  cáº¥u trÃºc mÃ´ hÃ¬nh Ä‘á»ƒ phÃ¹ há»£p vá»›i bÃ i toÃ¡n cá»¥ thá»ƒ cá»§a mÃ¬nh.


Äá»ƒ trÃ¬nh bÃ y chi tiáº¿t vÃ  mÃ£ nguá»“n Python sá»­ dá»¥ng Máº¡ng NÆ¡-ron DÃ i Háº¡n vÃ  Ngáº¯n Háº¡n (LSTM - Long Short-Term Memory) trong PyTorch cho bÃ i toÃ¡n phÃ¢n loáº¡i nhá»‹ phÃ¢n vÃ  phÃ¢n loáº¡i nhiá»u lá»›p, mÃ¬nh sáº½ cung cáº¥p cÃ¡c vÃ­ dá»¥ cá»¥ thá»ƒ.

### Binary Classification

#### Chuáº©n bá»‹ dá»¯ liá»‡u
ChÃºng ta sá»­ dá»¥ng dá»¯ liá»‡u tá»« thÆ° viá»‡n sklearn Ä‘á»ƒ minh há»a. ÄÃ¢y lÃ  má»™t vÃ­ dá»¥ vá»›i dá»¯ liá»‡u giáº£ láº­p:

```python
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
import numpy as np

# Táº¡o dá»¯ liá»‡u giáº£ láº­p
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)

# Chia dá»¯ liá»‡u thÃ nh táº­p huáº¥n luyá»‡n vÃ  táº­p kiá»ƒm tra
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Chuyá»ƒn Ä‘á»•i thÃ nh tensor trong PyTorch
import torch
from torch.utils.data import TensorDataset, DataLoader

X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.float32)

# Reshape láº¡i X Ä‘á»ƒ phÃ¹ há»£p vá»›i Ä‘áº§u vÃ o cá»§a LSTM (batch_size, seq_len, input_size)
X_train_tensor = X_train_tensor.view(-1, 20, 1)  # seq_len = 20, input_size = 1
X_test_tensor = X_test_tensor.view(-1, 20, 1)

# Táº¡o DataLoader cho táº­p huáº¥n luyá»‡n
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
```

#### XÃ¢y dá»±ng mÃ´ hÃ¬nh LSTM
Sá»­ dá»¥ng PyTorch, chÃºng ta cÃ³ thá»ƒ xÃ¢y dá»±ng mÃ´ hÃ¬nh LSTM nhÆ° sau:

```python
import torch.nn as nn
import torch.optim as optim

class LSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(LSTM, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
        self.sigmoid = nn.Sigmoid()
        
    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])  # Láº¥y output cá»§a lá»›p cuá»‘i cÃ¹ng
        out = self.sigmoid(out)
        return out

# Khá»Ÿi táº¡o mÃ´ hÃ¬nh
input_size = 1  # sá»‘ chiá»u cá»§a dá»¯ liá»‡u Ä‘áº§u vÃ o
hidden_size = 32  # sá»‘ nÆ¡-ron áº©n
num_layers = 1  # sá»‘ lá»›p LSTM
output_size = 1  # Ä‘áº§u ra cÃ³ 1 nÆ¡-ron vÃ¬ lÃ  bÃ i toÃ¡n nhá»‹ phÃ¢n
model = LSTM(input_size, hidden_size, num_layers, output_size)

# Äá»‹nh nghÄ©a hÃ m loss vÃ  optimizer
criterion = nn.BCELoss()  # Binary Cross Entropy Loss
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Huáº¥n luyá»‡n mÃ´ hÃ¬nh
num_epochs = 20

for epoch in range(num_epochs):
    model.train()
    epoch_loss = 0.0
    for inputs, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs.squeeze(), labels)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
    
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader):.4f}')

# ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh trÃªn táº­p kiá»ƒm tra
model.eval()
with torch.no_grad():
    X_test_tensor = X_test_tensor.view(-1, 20, 1)
    y_pred = model(X_test_tensor)
    y_pred_class = y_pred.round()
    accuracy = (y_pred_class.eq(y_test_tensor.view_as(y_pred_class)).sum() / len(y_test_tensor)).item()
    print(f'Accuracy on test set: {accuracy:.4f}')
```

### Multi-class Classification hoáº·c Regression (Prediction)

#### Chuáº©n bá»‹ dá»¯ liá»‡u
ChÃºng ta tiáº¿p tá»¥c sá»­ dá»¥ng dá»¯ liá»‡u tá»« thÆ° viá»‡n sklearn, nhÆ°ng cho má»™t vÃ­ dá»¥ vá»›i nhiá»u lá»›p:

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import numpy as np

# Load dá»¯ liá»‡u Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Chia dá»¯ liá»‡u thÃ nh táº­p huáº¥n luyá»‡n vÃ  táº­p kiá»ƒm tra
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Chuyá»ƒn Ä‘á»•i thÃ nh tensor trong PyTorch
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.long)  # y_train lÃ  index cá»§a lá»›p
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.long)

# Reshape láº¡i X Ä‘á»ƒ phÃ¹ há»£p vá»›i Ä‘áº§u vÃ o cá»§a LSTM (batch_size, seq_len, input_size)
X_train_tensor = X_train_tensor.view(-1, X.shape[1], 1)  # seq_len = sá»‘ Ä‘áº·c trÆ°ng cá»§a Iris, input_size = 1
X_test_tensor = X_test_tensor.view(-1, X.shape[1], 1)

# Táº¡o DataLoader cho táº­p huáº¥n luyá»‡n
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
```

#### XÃ¢y dá»±ng mÃ´ hÃ¬nh LSTM
```python
import torch.nn as nn
import torch.optim as optim

class LSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(LSTM, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
        
    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])  # Láº¥y output cá»§a lá»›p cuá»‘i cÃ¹ng
        return out

# Khá»Ÿi táº¡o mÃ´ hÃ¬nh
input_size = 1  # sá»‘ chiá»u cá»§a dá»¯ liá»‡u Ä‘áº§u vÃ o
hidden_size = 32  # sá»‘ nÆ¡-ron áº©n
num_layers = 1  # sá»‘ lá»›p LSTM
output_size = len(np.unique(y))  # sá»‘ lá»›p Ä‘áº§u ra
model = LSTM(input_size, hidden_size, num_layers, output_size)

# Äá»‹nh nghÄ©a hÃ m loss vÃ  optimizer
criterion = nn.CrossEntropyLoss()  # Cross Entropy Loss
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Huáº¥n luyá»‡n mÃ´ hÃ¬nh
num_epochs = 20

for epoch in range(num_epochs):
    model.train()
    epoch_loss = 0.0
    for inputs, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
    
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader):.4f}')

# ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh trÃªn táº­p kiá»ƒm tra
model.eval()
with torch.no_grad():
    X_test_tensor = X_test_tensor.view(-1, X.shape[1], 1)
    y_pred = model(X_test_tensor)
    _, y_pred_class = torch.max(y_pred, 1)
    accuracy = (y_pred_class.eq(y_test_tensor).sum() / len(y_test_tensor)).item()
    print(f'Accuracy on test set: {accuracy:.4f}')
```

### Tá»•ng káº¿t
TrÃªn Ä‘Ã¢y lÃ  cÃ¡ch sá»­ dá»¥ng Máº¡ng NÆ¡-ron DÃ i Háº¡n vÃ  Ngáº¯n Háº¡n (LSTM) trong PyTorch cho bÃ i toÃ¡n phÃ¢n loáº¡i nhá»‹ phÃ¢n vÃ  phÃ¢n loáº¡i nhiá»u lá»›p. MÃ£ nguá»“n Ä‘Ã£ cung cáº¥p bao gá»“m xÃ¢y dá»±ng mÃ´ hÃ¬nh, chuáº©n bá»‹ dá»¯ liá»‡u, Ä‘á»‹nh nghÄ©a hÃ m loss vÃ  optimizer, huáº¥n luyá»‡n mÃ´ hÃ¬nh vÃ  Ä‘Ã¡nh giÃ¡ káº¿t quáº£ trÃªn táº­p kiá»ƒm tra. Báº¡n cÃ³ thá»ƒ Ä‘iá»u chá»‰nh cÃ¡c tham sá»‘ vÃ  cáº¥u trÃºc mÃ´ hÃ¬nh Ä‘á»ƒ phÃ¹ há»£p vá»›i bÃ i toÃ¡n cá»¥ thá»ƒ cá»§a mÃ¬nh.




Háº¿t.
